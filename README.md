## Detoxify - Advanced toxic comment classification


Problem Statement: To build a multi-headed model capable of detecting different types of toxicity like threats, obscenity, insults, and identity-based hate.
A toxic comment is a comment that contains inappropriate or harmful language. For example, a comment that insults someone, threatens them or makes them feel bad about themselves is toxic.
This project aims to use deep learning to identify toxic comments. 

Firstly, I focused on data preprocessing and feature engineering to optimize the cleanliness of the dataset of 200,000+ comments.
Further, I preprocessed text data by tokenizing, padding, text normalization, and lemmatization to feed into the LSTM and LSTM-CNN models.
Lastly, I achieved a 95% accuracy in identifying toxic comments with a precision rate of 80% and a recall rate of 70%.


<img width="672" alt="Screenshot 2023-09-18 at 5 06 03 PM" src="https://github.com/Sarjak369/Detoxify/assets/56110199/8743d6bd-bd8e-4503-980b-093d1ad4c898">

<img width="620" alt="Screenshot 2023-09-18 at 5 06 17 PM" src="https://github.com/Sarjak369/Detoxify/assets/56110199/8201556b-8dcc-4382-9d4a-b7129adef6fb">
